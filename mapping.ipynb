{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-02-18T13:12:37.300040Z",
          "start_time": "2025-02-18T13:12:37.284416Z"
        },
        "id": "initial_id",
        "outputId": "2efb291f-13b1-4ec6-e86c-2fdcb8dc05fa"
      },
      "source": [
        "import itertools\n",
        "import string\n",
        "\n",
        "def generate_combinations(limit):\n",
        "    alphabets = list(string.ascii_lowercase[:limit])\n",
        "    combinations = []\n",
        "    for r in range(1, limit + 1):\n",
        "        combinations.extend(itertools.product(alphabets, repeat=r))\n",
        "\n",
        "    # Print all combinations\n",
        "    for combo in combinations:\n",
        "        print(\"\".join(combo))\n",
        "\n",
        "    print(\"Total combinations:\", len(combinations))\n",
        "\n",
        "generate_combinations(limit=3)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a\n",
            "b\n",
            "c\n",
            "aa\n",
            "ab\n",
            "ac\n",
            "ba\n",
            "bb\n",
            "bc\n",
            "ca\n",
            "cb\n",
            "cc\n",
            "aaa\n",
            "aab\n",
            "aac\n",
            "aba\n",
            "abb\n",
            "abc\n",
            "aca\n",
            "acb\n",
            "acc\n",
            "baa\n",
            "bab\n",
            "bac\n",
            "bba\n",
            "bbb\n",
            "bbc\n",
            "bca\n",
            "bcb\n",
            "bcc\n",
            "caa\n",
            "cab\n",
            "cac\n",
            "cba\n",
            "cbb\n",
            "cbc\n",
            "cca\n",
            "ccb\n",
            "ccc\n",
            "Total combinations: 39\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "id": "d729aa5a4f8eac93"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import string\n",
        "import corpus\n",
        "\n",
        "# Function to remove double letters\n",
        "def __remove_double_letters(word):\n",
        "    return \"\".join(ch for i, ch in enumerate(word) if i == 0 or ch != word[i - 1])\n",
        "\n",
        "# Function to simplify transliteration\n",
        "def simplify_transliteration(word):\n",
        "    exclude_word = {'h', 'H', 'w', 'W'}\n",
        "    exclude_cons = {'a', 'A', 'e', 'E', 'i', 'I', 'o', 'O', 'u', 'U', 'h', 'H', 'w', 'W', 'y', 'Y'}\n",
        "\n",
        "    word = __remove_double_letters(word)\n",
        "\n",
        "    simplified_word = ''\n",
        "    simplified_cons = ''\n",
        "\n",
        "    for index, char in enumerate(word):\n",
        "        if char.lower() == 'v':\n",
        "            char = 'b'\n",
        "        elif char.lower() == 'f':\n",
        "            char = 'p'\n",
        "        elif char.lower() == 'y':\n",
        "            char = 'i'\n",
        "        elif char.lower() == 'a':\n",
        "            char = 'e'\n",
        "        elif char.lower() in {'s', 'k'}:\n",
        "            char = 'c'\n",
        "        elif char.lower() == 'u':\n",
        "            char = 'o'\n",
        "        elif char.lower() in {'g', 'z'}:\n",
        "            char = 'j'\n",
        "\n",
        "        if index == 0:\n",
        "            simplified_word += char\n",
        "            simplified_cons += char\n",
        "        else:\n",
        "            if char not in exclude_word:\n",
        "                simplified_word += char\n",
        "\n",
        "            if char not in exclude_cons:\n",
        "                simplified_cons += char\n",
        "\n",
        "    return simplified_word, simplified_cons\n",
        "\n",
        "# Function to calculate Levenshtein distance\n",
        "def levenshtein_distance(s1, s2):\n",
        "    rows, cols = len(s1) + 1, len(s2) + 1\n",
        "    dp = [[0] * cols for _ in range(rows)]\n",
        "\n",
        "    for i in range(rows):\n",
        "        dp[i][0] = i\n",
        "    for j in range(cols):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, rows):\n",
        "        for j in range(1, cols):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
        "\n",
        "    return dp[-1][-1]\n",
        "\n",
        "# Function to find nearest words\n",
        "# def __find_nearest_words(word, n=3, lang=\"english\", include_same_distance_words=False):\n",
        "def __find_nearest_words(word, n, lang, include_same_distance_words=False):\n",
        "\n",
        "    word = word.lower()\n",
        "    word_sound, word_cons = simplify_transliteration(word)\n",
        "\n",
        "    nearest_words = []\n",
        "    unique_words = []\n",
        "    sound_distance_weight = 0.2\n",
        "    cons_distance_weight = 0.6\n",
        "    edit_distance_weight = 0.2\n",
        "\n",
        "    word_length = len(word)\n",
        "    ld = word_length\n",
        "    wl = len(word_cons) * 2\n",
        "    for dict_word, detail in corpus.corpus.items():\n",
        "\n",
        "    # for dict_word, detail in corpus.items():\n",
        "        if ('s' in word and 'k' in dict_word) or ('k' in word and 's' in dict_word):\n",
        "            continue\n",
        "\n",
        "        if levenshtein_distance(word, dict_word) > ld:\n",
        "            continue\n",
        "\n",
        "        if abs(len(word) - len(dict_word)) > wl:\n",
        "            continue  # Skip words with large length differences\n",
        "\n",
        "        dict_sound, dict_cons = simplify_transliteration(dict_word)\n",
        "\n",
        "        sound_distance = levenshtein_distance(word_sound, dict_sound)\n",
        "        cons_distance = levenshtein_distance(word_cons, dict_cons)\n",
        "        edit_distance = levenshtein_distance(word, dict_word)\n",
        "\n",
        "        combined_distance = (\n",
        "            (sound_distance_weight * sound_distance) +\n",
        "            (cons_distance_weight * cons_distance) +\n",
        "            (edit_distance_weight * edit_distance)\n",
        "        )\n",
        "\n",
        "        if lang == 'english':\n",
        "            nearest_words.append((dict_word, combined_distance))\n",
        "        else:\n",
        "            nearest_words.append((dict_word, combined_distance, detail['bangla'][0]))\n",
        "\n",
        "    nearest_words.sort(key=lambda x: x[1])  # Sort by distance\n",
        "\n",
        "    current_distance = 0\n",
        "    for nword, distance in nearest_words:\n",
        "        if nword not in unique_words:\n",
        "            if len(unique_words) < n:\n",
        "                unique_words.append(nword)\n",
        "                current_distance = distance\n",
        "            elif include_same_distance_words and current_distance == distance:\n",
        "                unique_words.append(nword)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return unique_words\n",
        "\n",
        "# Function to generate all letter combinations and find nearest words\n",
        "def generate_combinations(lower_limit, upper_limit):\n",
        "    alphabets = list(string.ascii_lowercase[:upper_limit])\n",
        "    combinations = []\n",
        "\n",
        "    for r in range(lower_limit, upper_limit + 1):\n",
        "        combinations.extend(itertools.product(alphabets, repeat=r))\n",
        "\n",
        "    for combo in combinations:\n",
        "        word = \"\".join(combo)\n",
        "        nearest_words = __find_nearest_words(word, 5, \"english\")\n",
        "        nearest_words_bn = __find_nearest_words(word, 5, \"english\")\n",
        "\n",
        "        print(f\"{word} -> en {nearest_words}, bn {nearest_words_bn}\")\n",
        "\n",
        "    print(\"Total combinations:\", len(combinations))\n",
        "\n",
        "# Run the function\n",
        "generate_combinations(lower_limit=2, upper_limit=3)"
      ],
      "id": "d729aa5a4f8eac93",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5974bea7f653ab60"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Final Draft 2 for corpus format\n",
        "import csv\n",
        "import json\n",
        "import unicodedata\n",
        "\n",
        "def soundex(query: str):\n",
        "    query = query.lower().strip()\n",
        "    letters = [char for char in query if char.isalpha()]\n",
        "\n",
        "    if len(letters) == 0:\n",
        "        return \"0000\"\n",
        "\n",
        "    first_letter = letters[0]\n",
        "    letters = letters[1:]\n",
        "    letters = [char for char in letters if char not in ('a', 'e', 'i', 'o', 'u', 'y', 'h', 'w')]\n",
        "\n",
        "    if len(letters) == 0:\n",
        "        return first_letter.upper() + \"000\"\n",
        "\n",
        "    to_replace = {\n",
        "        ('b', 'f', 'p', 'v'): 1,\n",
        "        ('c', 'g', 'j', 'k', 'q', 's', 'x', 'z'): 2,\n",
        "        ('d', 't'): 3,\n",
        "        ('l',): 4,\n",
        "        ('m', 'n'): 5,\n",
        "        ('r',): 6\n",
        "    }\n",
        "\n",
        "    first_letter_digit = next((value for group, value in to_replace.items() if first_letter in group), first_letter)\n",
        "    letters = [next((value for group, value in to_replace.items() if char in group), char) for char in letters]\n",
        "    letters = [char for i, char in enumerate(letters) if i == 0 or char != letters[i - 1]]\n",
        "\n",
        "    if isinstance(first_letter_digit, int) and first_letter_digit == letters[0]:\n",
        "        letters[0] = query[0]\n",
        "    else:\n",
        "        letters.insert(0, first_letter)\n",
        "\n",
        "    letters = [char for char in letters if isinstance(char, int)][:3]\n",
        "    while len(letters) < 3:\n",
        "        letters.append(0)\n",
        "\n",
        "    return str(first_letter).upper() + ''.join(map(str, letters))\n",
        "\n",
        "\n",
        "def csv_to_dict(csv_files):\n",
        "    DICTIONARY = {}\n",
        "    sl = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        with open(csv_file, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "\n",
        "            for row in reader:\n",
        "                if row.get('Name?', '').strip().lower() == 'y':\n",
        "                    continue\n",
        "                key = row['Corrected'].strip() if row['Corrected'].strip() else row['English'].strip().lower()\n",
        "                sl += 1\n",
        "                bangla_value = unicodedata.normalize('NFC', row['Bangla'].strip())\n",
        "\n",
        "                try:\n",
        "                    soundex_code = soundex(key)\n",
        "                except IndexError as e:\n",
        "                    print(f\"Error processing key '{key}' at entry {sl}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                if key in DICTIONARY:\n",
        "                    normalized_existing = [unicodedata.normalize('NFC', val) for val in DICTIONARY[key]['bangla']]\n",
        "                    if bangla_value not in normalized_existing:\n",
        "                        DICTIONARY[key]['bangla'].append(bangla_value)\n",
        "                else:\n",
        "                    DICTIONARY[key] = {\n",
        "                        'bangla': [bangla_value]\n",
        "                    }\n",
        "\n",
        "    return DICTIONARY\n",
        "\n",
        "\n",
        "csv_files = [\n",
        "    '/content/bangla_scraped_words_v2 - 10k_1.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_2.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_3.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_4.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_5.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_6.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_7.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_8.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_9.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_10.csv',\n",
        "    '/content/bangla_scraped_words_v2 - old_10k_1.csv'\n",
        "]\n",
        "\n",
        "combined_dictionary_output = csv_to_dict(csv_files)\n",
        "\n",
        "print(json.dumps(combined_dictionary_output, indent=4, ensure_ascii=False))\n",
        "\n",
        "file_path = \"/content/corpus.py\"\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(f\"dictionary={json.dumps(combined_dictionary_output, ensure_ascii=False, separators=(',', ':'))}\")\n"
      ],
      "id": "5974bea7f653ab60"
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus_merged.json\n",
        "import csv\n",
        "import json\n",
        "\n",
        "csv_files = [\n",
        "    '/content/bangla_scraped_words_v2 - 10k_1.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_2.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_3.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_4.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_5.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_6.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_7.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_8.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_9.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_10.csv',\n",
        "    '/content/bangla_scraped_words_v2 - old_10k_1.csv'\n",
        "]\n",
        "\n",
        "# Final corpus dictionary: { \"key_word\": \"bangla_word1_bangla_word2\" }\n",
        "corpus_merged = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    try:\n",
        "        with open(csv_file, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                if row.get('Name?', '').strip().lower() == 'y':\n",
        "                    continue\n",
        "\n",
        "                key = row.get('Corrected', '').strip()\n",
        "                if not key:\n",
        "                    key = row.get('English', '').strip().lower()\n",
        "\n",
        "                bangla = row.get('Bangla', '').strip()\n",
        "\n",
        "                if key and bangla:\n",
        "                    if key not in corpus_merged:\n",
        "                        corpus_merged[key] = bangla\n",
        "                    else:\n",
        "                        existing = corpus_merged[key].split('_')\n",
        "                        if bangla not in existing:\n",
        "                            corpus_merged[key] += f'_{bangla}'\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process file {csv_file}: {e}\")\n",
        "\n",
        "corpus_merged_path = '/content/corpus_merged.json'\n",
        "with open(corpus_merged_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(corpus_merged, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Corpus saved to: {corpus_merged_path}\")\n"
      ],
      "metadata": {
        "id": "DJEIYNf0GJpd"
      },
      "id": "DJEIYNf0GJpd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus_en.json\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Input CSV files\n",
        "csv_files = [\n",
        "    '/content/bangla_scraped_words_v2 - 10k_1.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_2.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_3.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_4.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_5.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_6.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_7.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_8.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_9.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_10.csv',\n",
        "    '/content/bangla_scraped_words_v2 - old_10k_1.csv'\n",
        "]\n",
        "\n",
        "unique_words_ordered = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    try:\n",
        "        with open(csv_file, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                if row.get('Name?', '').strip().lower() == 'y':\n",
        "                    continue\n",
        "                word = row['Corrected'].strip() if row['Corrected'].strip() else row['English'].strip().lower()\n",
        "                if word and word not in unique_words_ordered:\n",
        "                    unique_words_ordered[word] = None\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process file {csv_file}: {e}\")\n",
        "\n",
        "corpus_en_dict = {str(i): word for i, word in enumerate(unique_words_ordered.keys())}\n",
        "\n",
        "corpus_en_path = '/content/corpus_en.json'\n",
        "with open(corpus_en_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(corpus_en_dict, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Saved unique ordered corpus to: {corpus_en_path}\")\n"
      ],
      "metadata": {
        "id": "d4CveKqRGU2r"
      },
      "id": "d4CveKqRGU2r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing bn words in csv\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_token(token):\n",
        "    # Remove \"৷\", \"ঃ\", and any whitespace characters\n",
        "    return re.sub(r\"[৷ঃ\\s]+\", \"\", token)\n",
        "\n",
        "def tokenize_bangla_text(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    tokens = re.findall(r'[\\u0980-\\u09FF]+', text)\n",
        "    return [clean_token(token) for token in tokens if clean_token(token)]\n",
        "\n",
        "sentiment_df = pd.read_csv('/content/Sentiment Analysis Bangla.csv')\n",
        "\n",
        "all_comment_words = set()\n",
        "for sentence in sentiment_df['comment'].dropna():\n",
        "    all_comment_words.update(tokenize_bangla_text(sentence))\n",
        "\n",
        "csv_files = [\n",
        "    '/content/bangla_scraped_words_v2 - 10k_1.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_2.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_3.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_4.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_5.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_6.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_7.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_8.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_9.csv',\n",
        "    '/content/bangla_scraped_words_v2 - 10k_10.csv',\n",
        "    '/content/bangla_scraped_words_v2 - old_10k_1.csv'\n",
        "]\n",
        "\n",
        "known_bangla_words = set()\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "    if 'Bangla' in df.columns:\n",
        "        words = df['Bangla'].dropna().astype(str).apply(lambda w: clean_token(w.strip()))\n",
        "        known_bangla_words.update(words)\n",
        "\n",
        "missing_words = sorted(all_comment_words - known_bangla_words)\n",
        "\n",
        "print(\"Number of missing words:\", len(missing_words))\n",
        "print(\"Sample missing words:\", missing_words[:20])\n",
        "\n",
        "pd.DataFrame(missing_words, columns=['Missing_Bangla_Words']).to_csv('missing_bangla_words.csv', index=False)\n"
      ],
      "metadata": {
        "id": "S_5twQ_uHa4E"
      },
      "id": "S_5twQ_uHa4E",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}